ipynbviewer
agentic_rag_azure.ipynb
Notebook URL
GitHub
Agentic RAG System with Azure OpenAI & LangGraph
Assignment 3 â€“ Jupyter Notebook Solution
# Install required packages (uncomment if needed)
# !pip install langgraph azure-ai-inference pinecone-client mlflow pydantic openai
import os
import json
import mlflow
from azure.ai.inference import EmbeddingsClient, OpenAIClient
import pinecone
from langgraph import Graph, Node
# Set environment variables
os.environ['AZURE_EMBEDDINGS_ENDPOINT'] = 'YOUR_AZURE_EMBEDDINGS_ENDPOINT'
os.environ['AZURE_EMBEDDINGS_KEY'] = 'YOUR_AZURE_EMBEDDINGS_KEY'
os.environ['AZURE_OPENAI_ENDPOINT'] = 'YOUR_AZURE_OPENAI_ENDPOINT'
os.environ['AZURE_OPENAI_KEY'] = 'YOUR_AZURE_OPENAI_KEY'
os.environ['PINECONE_API_KEY'] = 'YOUR_PINECONE_API_KEY'
# Load KB JSON
with open('self_critique_loop_dataset.json', 'r') as f:
    kb_entries = json.load(f)

# Azure Embeddings setup
embeddings_client = EmbeddingsClient(
    endpoint=os.environ['AZURE_EMBEDDINGS_ENDPOINT'],
    api_key=os.environ['AZURE_EMBEDDINGS_KEY'],
    model="text-embedding-3-small"
)

# Pinecone setup
pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment='gcp-starter')
index = pinecone.Index('kb-index')

# Indexing KB
for entry in kb_entries:
    text = entry['content']
    embedding = embeddings_client.embed(text)
    index.upsert([(entry['id'], embedding, {'text': text})])

print("Indexing complete.")
# Retriever Node
def retrieve_snippets(query, top_k=5):
    embedding = embeddings_client.embed(query)
    results = index.query(embedding, top_k=top_k, include_metadata=True)
    snippets = [r['metadata']['text'] for r in results['matches']]
    ids = [r['id'] for r in results['matches']]
    return snippets, ids
# LLM Answer Node
openai_client = OpenAIClient(
    endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],
    api_key=os.environ['AZURE_OPENAI_KEY'],
    deployment_name="gpt-4-mini"
)

def generate_answer(snippets, query):
    prompt = f"Answer the question using the following KB snippets (cite as [KBxxx]):\n"
    for i, s in enumerate(snippets):
        prompt += f"[KB{i+1}] {s}\n"
    prompt += f"\nQuestion: {query}\nAnswer:"
    response = openai_client.complete(prompt, temperature=0)
    return response['choices'][0]['text']
# Self-Critique Node
def self_critique(answer):
    if "enough" in answer.lower():
        return "COMPLETE"
    else:
        return "REFINE"
# Refinement Node
def refinement(query, used_ids):
    embedding = embeddings_client.embed(query)
    results = index.query(embedding, top_k=6, include_metadata=True)
    for r in results['matches']:
        if r['id'] not in used_ids:
            return r['metadata']['text'], r['id']
    return None, None
# Agentic RAG Pipeline with MLflow
def agentic_rag_pipeline(query):
    mlflow.start_run()
    snippets, ids = retrieve_snippets(query)
    mlflow.log_param("retrieved_snippets", snippets)
    answer = generate_answer(snippets, query)
    mlflow.log_param("initial_answer", answer)
    critique = self_critique(answer)
    mlflow.log_param("critique", critique)
    if critique == 'COMPLETE':
        mlflow.log_param("final_answer", answer)
        mlflow.end_run()
        return answer
    else:
        new_snippet, new_id = refinement(query, ids)
        all_snippets = snippets + [new_snippet]
        answer = generate_answer(all_snippets, query)
        mlflow.log_param("refined_answer", answer)
        mlflow.end_run()
        return answer
# Sample Queries
sample_queries = [
    "What are best practices for caching?",
    "How should I set up CI/CD pipelines?",
    "What are performance tuning tips?",
    "How do I version my APIs?",
    "What should I consider for error handling?"
]

for q in sample_queries:
    print(f"Q: {q}")
    print(agentic_rag_pipeline(q))
    print("-" * 40)
